{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMAbQ6kU77c63mW3lVjtf1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monkrus/pytorch_example/blob/main/pytorch_bert_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code provides a complete example of using BERT for text classification, from dataset preparation to fine-tuning and making predictions. You can customize the dataset, model parameters, and other settings to fit your specific use case or due to the warning."
      ],
      "metadata": {
        "id": "YB7QkuRVOqJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install libraries\n",
        "!pip install torch transformers"
      ],
      "metadata": {
        "id": "CVIB3Cw6Jlnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare dataset and dataloader\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Example dataset\n",
        "data = {\n",
        "    'text': [\n",
        "        'I love this movie!',\n",
        "        'This film was terrible.',\n",
        "        'What a fantastic performance!',\n",
        "        'I did not enjoy the plot.',\n",
        "        'The acting was okay, but the story was boring.',\n",
        "        'Great movie with an excellent cast.',\n",
        "        'Worst movie I have ever seen.',\n",
        "        'Absolutely loved the direction and the screenplay.',\n",
        "        'Not my cup of tea.',\n",
        "        'An enjoyable experience overall.'\n",
        "    ],\n",
        "    'label': [1, 0, 1, 0, 0, 1, 0, 1, 0, 1]  # 1 for positive, 0 for negative\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Load pre-trained BERT tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {'input_ids': inputs['input_ids'].squeeze(), 'attention_mask': inputs['attention_mask'].squeeze(), 'label': torch.tensor(label, dtype=torch.long)}\n",
        "\n",
        "# Create dataset objects\n",
        "train_dataset = TextDataset(train_texts.tolist(), train_labels.tolist(), tokenizer, max_length=64)\n",
        "val_dataset = TextDataset(val_texts.tolist(), val_labels.tolist(), tokenizer, max_length=64)\n",
        "\n",
        "# Create dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n"
      ],
      "metadata": {
        "id": "t0t7Dn-QJnIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fine-tune BERT\n",
        "# Load pre-trained BERT model\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "num_epochs = 3\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation loss: {avg_val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "iCoIKs2rKDtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making predictions\n",
        "# Load the fine-tuned model and tokenizer\n",
        "fine_tuned_model = BertForSequenceClassification.from_pretrained('fine-tuned-bert')\n",
        "fine_tuned_tokenizer = BertTokenizer.from_pretrained('fine-tuned-bert')\n",
        "fine_tuned_model.to(device)\n",
        "\n",
        "# Predict on new data\n",
        "def predict(text):\n",
        "    inputs = fine_tuned_tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=64,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = fine_tuned_model(input_ids, attention_mask=attention_mask)\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    predicted_class = torch.argmax(probabilities).item()\n",
        "    return predicted_class\n",
        "\n",
        "# Example predictions\n",
        "texts = [\"I really enjoyed this movie.\", \"The plot was boring and predictable.\"]\n",
        "predictions = [predict(text) for text in texts]\n",
        "print(predictions)  # Output: [1, 0]\n"
      ],
      "metadata": {
        "id": "8npZ8MgxLULV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The result is [1] positive, [0] negative**"
      ],
      "metadata": {
        "id": "nojEBB_-MfEF"
      }
    }
  ]
}